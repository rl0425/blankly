/**
 * Validator Agent - Self-critique ì´ì¤‘ ê²€ì¦
 * 
 * ëª©ì :
 * - Self-critique ê³¼ì‹  ë°©ì§€
 * - í’ˆì§ˆ ë‚®ì€ ë¬¸ì œ í•„í„°ë§
 * - 20% ìƒ˜í”Œë§ìœ¼ë¡œ ë¹„ìš© ìµœì†Œí™”
 */

import { generateWithGPT4o } from '@/shared/lib/openai/client';
import type { GeneratedProblem } from '@/shared/lib/prompts/base';

export interface ValidationResult {
  actual_score: number;
  issues: string[];
  recommendation: 'accept' | 'reject' | 'revise';
}

/**
 * ë¬¸ì œ ê²€ì¦ (ì—„ê²©í•œ í‰ê°€)
 */
export async function validateProblem(problem: GeneratedProblem): Promise<ValidationResult> {
  const prompt = `You are a HARSH exam question reviewer. Most problems are 5-7 quality range.

Rate this problem STRICTLY:
1. Is the correct answer unambiguous? (0-10)
2. Are distractors plausible but clearly wrong? (0-10)
3. Is this real exam-grade quality? (0-10)
4. Is the explanation clear and includes the answer? (0-10)

Problem to review:
${JSON.stringify(problem, null, 2)}

Output JSON only:
{
  "actual_score": 1-10 (average of above ratings),
  "issues": ["specific issue 1", "specific issue 2", ...],
  "recommendation": "accept" | "reject" | "revise"
}

Be harsh - only exceptional problems should score 9-10.`;
  
  try {
    const result = await generateWithGPT4o({
      systemPrompt: 'You are a strict quality validator. Be harsh and critical.',
      userPrompt: prompt,
      temperature: 0.3,
      responseFormat: 'json_object',
      stage: 'validation',
      maxTokens: 500,
    });
    
    const validation = JSON.parse(result.content || '{}');
    return {
      actual_score: validation.actual_score || 0,
      issues: validation.issues || [],
      recommendation: validation.recommendation || 'reject',
    };
  } catch (error) {
    console.error('Validation failed:', error);
    // ê²€ì¦ ì‹¤íŒ¨ ì‹œ ë³´ìˆ˜ì ìœ¼ë¡œ reject
    return {
      actual_score: 0,
      issues: ['Validation service failed'],
      recommendation: 'reject',
    };
  }
}

/**
 * ìƒ˜í”Œë§ ê¸°ë°˜ ë¬¸ì œ ê²€ì¦
 * 
 * ì „ëµ:
 * - Low score (< 8) ë¬¸ì œ: 100% ê²€ì¦
 * - ë‚˜ë¨¸ì§€: 20% ë¬´ì‘ìœ„ ìƒ˜í”Œë§
 */
export async function validateProblemsWithSampling(
  problems: GeneratedProblem[]
): Promise<{ validated: GeneratedProblem[]; rejected: number }> {
  console.log('ğŸ” Starting validator agent (sampling mode)...');
  
  // 1. Low score ë¬¸ì œ ì„ ë³„
  const lowScoreProblems = problems.filter(
    p => p.self_critique && p.self_critique.quality_score < 8
  );
  
  // 2. ë‚˜ë¨¸ì§€ì—ì„œ 20% ìƒ˜í”Œë§
  const highScoreProblems = problems.filter(
    p => !p.self_critique || p.self_critique.quality_score >= 8
  );
  const sampledProblems = highScoreProblems.filter(() => Math.random() < 0.2);
  
  // 3. ê²€ì¦ ëŒ€ìƒ (ì¤‘ë³µ ì œê±°)
  const toValidate = [...new Set([...lowScoreProblems, ...sampledProblems])];
  
  console.log(`  ğŸ“Š Validating ${toValidate.length}/${problems.length} problems`);
  console.log(`    - Low score: ${lowScoreProblems.length}`);
  console.log(`    - Sampled: ${sampledProblems.length}`);
  
  // 4. ê²€ì¦ ì‹¤í–‰
  const rejectedIds = new Set<GeneratedProblem>();
  
  for (const problem of toValidate) {
    const validation = await validateProblem(problem);
    
    if (validation.actual_score < 7 || validation.recommendation === 'reject') {
      rejectedIds.add(problem);
      console.warn(`  âŒ Validator rejected problem:`, {
        selfScore: problem.self_critique?.quality_score,
        validatorScore: validation.actual_score,
        issues: validation.issues,
      });
    }
  }
  
  // 5. í†µê³¼í•œ ë¬¸ì œë§Œ ë°˜í™˜
  const validated = problems.filter(p => !rejectedIds.has(p));
  
  console.log(`  âœ… Validation complete: ${validated.length}/${problems.length} passed`);
  
  return {
    validated,
    rejected: rejectedIds.size,
  };
}


